{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "izMm_DD0MxdM"
      },
      "source": [
        "# **INSTALL simpletransformers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dv6ysI_Mvam"
      },
      "outputs": [],
      "source": [
        "!pip install simpletransformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ15NmOTNGQG"
      },
      "source": [
        "# **IMPORT LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdnZgSK-LsR3"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "np.random.seed(0)\n",
        "plt.style.use(\"ggplot\")\n",
        "import tensorflow as tf\n",
        "from simpletransformers.ner import NERModel, NERArgs\n",
        "\n",
        "print('Tensorflow version:', tf.__version__)\n",
        "print('GPU detected:', tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyZ-w8mzNTg7"
      },
      "source": [
        "# **LOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qpBU9BSNOnf"
      },
      "outputs": [],
      "source": [
        "# Efficiently load training, dev, and test data sets for train and optimal model evaluation\n",
        "\n",
        "train = pd.read_csv('data/Sequence_labeling_based_version/Word/train_BIO_Word.csv')\n",
        "dev = pd.read_csv('data/Sequence_labeling_based_version/Word/dev_BIO_Word.csv')\n",
        "test = pd.read_csv('data/Span Extraction-based version/test.csv')\n",
        "\n",
        "# Delete redundant column\n",
        "train.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "dev.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\n",
        "test.drop(columns=['Unnamed: 0'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygj44d9AOWpf"
      },
      "source": [
        "# **BASIC PROCESS DATA BEFORE TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CGHAK90ORij"
      },
      "outputs": [],
      "source": [
        "# Fill Null vale\n",
        "train.replace(np.nan, \"NULL\", inplace=True)\n",
        "dev.replace(np.nan, \"NULL\", inplace=True)\n",
        "test.replace(np.nan, \"NULL\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mglICnt8OfaH"
      },
      "outputs": [],
      "source": [
        "# Get number of tag in training dataset\n",
        "tags = list(set(train[\"Tag\"].values))\n",
        "num_tags = len(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb53mfQdO1Ps"
      },
      "outputs": [],
      "source": [
        "# Convert training set and dev set to Simpletransformer format\n",
        "train = pd.DataFrame({'sentence_id': train['sentence_id'],'words': train['Word'], 'labels': train['Tag']})\n",
        "dev = pd.DataFrame({'sentence_id': dev['sentence_id'],'words': dev['Word'], 'labels': dev['Tag']})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izcUwHPGPGvl"
      },
      "source": [
        "# **TRAINING MODEL**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model should be trained multiple times with different random seeds to get the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prxmsoa0PBWB"
      },
      "outputs": [],
      "source": [
        "# Set up param for model\n",
        "\n",
        "args = NERArgs()\n",
        "args.num_train_epochs = 10\n",
        "args.learning_rate = 2e-5\n",
        "args.overwrite_output_dir =True\n",
        "args.train_batch_size = 10\n",
        "args.eval_batch_size = 10\n",
        "args.use_cached_eval_features = False\n",
        "args.use_multiprocessing = False\n",
        "args.reprocess_input_data = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEkB8DmtPRcV"
      },
      "outputs": [],
      "source": [
        "# Load model XLMR large \n",
        "model = NERModel(\"auto\", \"vinai/phobert-large\", labels=tags, args=args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIGfaVrLPaQ9"
      },
      "outputs": [],
      "source": [
        "# Training model\n",
        "model.train_model(train, validation_data=dev, acc=accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(test)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlO8_C4jPeO-"
      },
      "source": [
        "# **EVALUATE AND PREDICT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTD6hThSPrbf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ast import literal_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR6zUeFDPkqc"
      },
      "outputs": [],
      "source": [
        "# Predict follow IBO format for test data\n",
        "\n",
        "test['index_spans'] = test['index_spans'].apply(literal_eval)\n",
        "\n",
        "\n",
        "def IBO_pred(tokens):\n",
        "  ibo_list = []\n",
        "  value_list = []\n",
        "  pred = model.predict(tokens)\n",
        "  for i in range(len(pred[0])):\n",
        "    ibo_pred = pred[0][i][0]\n",
        "    value_pred = list(ibo_pred.keys())\n",
        "    ibo_pred = list(ibo_pred.values())\n",
        "    ibo_list.append(ibo_pred[0])\n",
        "    value_list.append(value_pred[0])\n",
        "  return value_list, ibo_list\n",
        "\n",
        "def IBO_pred_test(test):\n",
        "  list_ibo = []\n",
        "  list_value =[]\n",
        "  df_pred = pd.DataFrame()\n",
        "  for i in range(len(test)):\n",
        "    text = test['content'][i]\n",
        "    tokens = text.split()\n",
        "    ibo_, t = IBO_pred(tokens)\n",
        "    list_ibo.append(ibo_)\n",
        "    list_value.append(t)\n",
        "  df_pred['Value_pred'] = list_value\n",
        "  df_pred['IBO_pred'] = list_ibo\n",
        "  return df_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwJ_CND8P8KN"
      },
      "outputs": [],
      "source": [
        "# Predict IBO task for each comment in test data\n",
        "\n",
        "df_ibo = IBO_pred_test(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz0ssCCGQFk9"
      },
      "outputs": [],
      "source": [
        "# Get token predict and token correct \n",
        "\n",
        "def tokenize(text, pos):\n",
        "    tokens = text.split()\n",
        "    alignment = []\n",
        "    start = 0\n",
        "    for t in tokens:\n",
        "        res = text.find(t, start)\n",
        "        alignment.append(pos[res:res + len(t)])\n",
        "        start = res + len(t)\n",
        "    assert len(tokens) == len(alignment)\n",
        "    return tokens, alignment\n",
        "def y_pred(data, df_predict):\n",
        "  index_pred = []\n",
        "  for i in range(len(df_predict)):\n",
        "    value_predict_i = df_predict['Value_pred'][i]\n",
        "    text = data['content'][i]\n",
        "    pos = [i for i in range(len(text))]\n",
        "    tokens, alignment = tokenize(text, pos)\n",
        "    df_point = pd.DataFrame()\n",
        "    df_point['spans'] = pos\n",
        "    df_point['spans'] = 0\n",
        "    for i, token in enumerate(value_predict_i):\n",
        "      if token == 'B-T' or token == 'I-T':\n",
        "        for ali in alignment[i]:\n",
        "          df_point['spans'][ali] = 1\n",
        "    index_pred.append(list(df_point['spans']))\n",
        "  return index_pred\n",
        "def y_true(data):\n",
        "  index_true = []\n",
        "  for i in range(len(data)):\n",
        "    text = data['content'][i]\n",
        "    pos = [i for i in range(len(text))]\n",
        "    df_point = pd.DataFrame()\n",
        "    df_point['spans'] = pos\n",
        "    df_point['spans'] = 0\n",
        "    if not data['index_spans'][i]:\n",
        "      index_true.append(list(df_point['spans']))\n",
        "    else:\n",
        "      for j in data['index_spans'][i]:\n",
        "        df_point['spans'][j] = 1\n",
        "      index_true.append(list(df_point['spans']))\n",
        "  return index_true\n",
        "\n",
        "true = y_true(test)\n",
        "pred = y_pred(test, df_ibo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUkdkvUAQdZk"
      },
      "source": [
        "# **SCORE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1_lH_D3Qhnt"
      },
      "outputs": [],
      "source": [
        "# Dataframe for save evaluation metrics\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "scores_f1_macro = []\n",
        "scores_f1_micro = []\n",
        "scores_precision_macro = []\n",
        "scores_precision_micro = []\n",
        "scores_recall_macro = []\n",
        "scores_recall_micro = []\n",
        "\n",
        "for i in range(len(true)):\n",
        "  score_macro = precision_recall_fscore_support(true[i], pred[i], average='macro')\n",
        "  score_micro = precision_recall_fscore_support(true[i], pred[i], average='micro')\n",
        "\n",
        "  scores_f1_macro.append(score_macro[2])\n",
        "  scores_f1_micro.append(score_micro[2])\n",
        "  scores_precision_macro.append(score_macro[0])\n",
        "  scores_precision_micro.append(score_micro[0])\n",
        "  scores_recall_macro.append(score_macro[1])\n",
        "  scores_recall_micro.append(score_micro[1])\n",
        "\n",
        "scores = pd.DataFrame()\n",
        "scores['eval_loss'] = [list(result.values())[0]]\n",
        "scores['F1_ner'] = [list(result.values())[1]]\n",
        "scores['F1-micro'] = [np.mean(scores_f1_micro)]\n",
        "scores['F1-macro'] = [np.mean(scores_f1_macro)]\n",
        "scores['Precision-macro'] = [np.mean(scores_precision_macro)]\n",
        "scores['Precision-micro'] = [np.mean(scores_precision_micro)]\n",
        "scores['Recall-macro'] = [np.mean(scores_recall_macro)]\n",
        "scores['Recall-micro'] = [np.mean(scores_recall_micro)]\n",
        "\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a48QJzaeQwpx"
      },
      "source": [
        "# **ERROR DATAFRAME FOR EVALUATE RESULT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRLbimxzRCUK"
      },
      "outputs": [],
      "source": [
        "def word_true(df):\n",
        "  list_toxic_true = []\n",
        "  for idx in range(len(test)):\n",
        "    toxic_true = []\n",
        "    m = df.loc[df['sentence_id'] == idx]\n",
        "    m.reset_index(inplace=True)\n",
        "    k = list(m['words'])\n",
        "    for i in range(len(k)):\n",
        "      if(i == (len(k) - 1)):\n",
        "        if(m['labels'][i] == 'B-T'):\n",
        "          toxic_true.append(m['words'][i])\n",
        "        elif(m['labels'][i] == 'O'):\n",
        "          continue\n",
        "      else:\n",
        "        if(m['labels'][i] == 'B-T' and m['labels'][i+1] == 'O'):\n",
        "          toxic_true.append(m['words'][i])\n",
        "        elif(m['labels'][i] == 'B-T' and m['labels'][i+1] == 'B-T'):\n",
        "          toxic_true.append(m['words'][i])\n",
        "        elif(m['labels'][i] == 'B-T' and m['labels'][i+1] == 'I-T'):\n",
        "          j = i + 1\n",
        "          list_word = m['words'][i]\n",
        "          while(m['labels'][j] == 'I-T'):\n",
        "            list_word = list_word + \" \" + m['words'][j]\n",
        "            if(j == (len(k) -1 )):\n",
        "              break;\n",
        "            else:\n",
        "              j = j + 1\n",
        "          toxic_true.append(list_word)\n",
        "    list_toxic_true.append(toxic_true) \n",
        "  return list_toxic_true\n",
        "def word_pred(df):\n",
        "  pre_value =[]\n",
        "  for i in range(len(df)):\n",
        "    temp = []\n",
        "    for j in range(len(df['Value_pred'][i])):\n",
        "      if df['Value_pred'][i][j] == 'B-T' or df['Value_pred'][i][j] == 'I-T':\n",
        "        temp.append(df['IBO_pred'][i][j])\n",
        "    pre_value.append(temp)\n",
        "  return pre_value\n",
        "\n",
        "\n",
        "\n",
        "# Error dataframe\n",
        "error = pd.DataFrame()\n",
        "error['True'] = word_true(test)\n",
        "error['Pred'] = word_pred(df_ibo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z23gn064Raov"
      },
      "source": [
        "# **SAVE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buuHSygXRZ8I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.save(model, 'model.pt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
